
# ML Pipeline Project

A robust, modular, and production-ready machine learning pipeline that automates data ingestion, validation, transformation, training, evaluation, and deployment. This project integrates **MLflow** for experiment tracking and **DagsHub** for version control and artifact management.

---

## Key Features

- Modular pipeline with clearly defined stages
- **MLflow** integration for tracking experiments, metrics, and parameters
- **DagsHub** support for remote artifact storage and Git-based tracking
- YAML-based configuration for reproducibility
- Schema validation and data quality checks
- Model evaluation with automatic comparison and promotion logic
- Packaged with Docker for scalable deployment
- Flask web interface for real-time predictions and triggering training

---

## Project Structure

```
ML_Pipeline/
â”œâ”€â”€ main.py                      # Executes the full training pipeline
â”œâ”€â”€ app.py                       # Flask web app for UI interaction
â”œâ”€â”€ Dockerfile                   # Docker build setup
â”œâ”€â”€ params.yaml                  # Configurations for the pipeline
â”œâ”€â”€ schema.yaml                  # Expected input schema
â”œâ”€â”€ model.pkl                    # Output: trained model artifact
â”œâ”€â”€ src/mlproject/
â”‚   â”œâ”€â”€ components/              # Core logic for pipeline stages
â”‚   â”œâ”€â”€ pipelines/               # Entry points for each pipeline stage
â”‚   â”œâ”€â”€ config/                  # YAML loader and configuration manager
â”‚   â”œâ”€â”€ entity/                  # Configuration data classes
â”‚   â”œâ”€â”€ constants/               # Paths and constants
```

---

## Pipeline Stages

### 1. Data Ingestion
- Downloads or loads dataset
- Saves raw data into artifact directory

### 2. Data Validation
- Validates input data schema (`schema.yaml`)
- Logs results to console and MLflow

### 3. Data Transformation
- Converts raw data into train-ready format
- Saves processed datasets

### 4. Model Training
- Trains a supervised ML model
- Logs metrics and parameters to MLflow
- Stores final model as `model.pkl`

### 5. Model Evaluation
- Compares with existing model
- If better, logs and promotes it
- Tracks all metrics via MLflow and artifacts via DagsHub

---

## Web App Interface

Flask endpoints:

- `/` â€” Homepage
- `/train` â€” Triggers full pipeline
- `/predict` â€” Accepts 11 features via HTML form and returns prediction

### Features Required for Prediction:

- fixed_acidity, volatile_acidity, citric_acid, residual_sugar
- chlorides, free_sulfur_dioxide, total_sulfur_dioxide
- density, pH, sulphates, alcohol

---

## MLflow + DagsHub Integration

- Logs experiments to `mlruns/` directory or remote DagsHub tracking URI
- Tracks:
  - Hyperparameters
  - Metrics (accuracy, loss, etc.)
  - Artifacts (models, plots, etc.)
- Version control handled via Git/DVC and DagsHub

To set MLflow tracking URI:

```bash
export MLFLOW_TRACKING_URI=https://dagshub.com/<user>/<repo>.mlflow
```

---

## Docker Setup

```bash
docker build -t ml-pipeline .
docker run -p 5000:5000 ml-pipeline
```

---

## ğŸ› ï¸ How to Run

### 1. Install dependencies

```bash
pip install -r requirements.txt
```

### 2. Run training

```bash
python main.py
```

### 3. Start web app

```bash
python app.py
```

---

## Config Files

- `params.yaml` â€” model parameters, data paths
- `schema.yaml` â€” defines required schema for validation
- `config/config.yaml` â€” supports remote paths and artifact storage

